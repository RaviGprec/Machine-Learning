{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp text classification_part_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaviGprec/Machine-Learning/blob/master/nlp_text_classification_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oSgQvDRG0wR",
        "colab_type": "text"
      },
      "source": [
        "#Text Classification \n",
        "\n",
        "Reference:\n",
        "[https://github.com/miguelfzafra/Latest-News-Classifier](https://github.com/miguelfzafra/Latest-News-Classifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7HvUu0LCzCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import punkt\n",
        "from nltk.corpus.reader import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rKmqKZ43ILh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "# Code for hiding seaborn warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle\n",
        "#df = pd.read_csv(\"News_dataset.csv\",sep = \";\", encoding = \"utf-8\")\n",
        "#df.head()\n",
        "\n",
        "with open(\"News_dataset.pickle\", 'rb') as data:\n",
        "    df = pickle.load(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drEOnQdg6DpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "afccd294-21cb-4129-b99b-6270376059d0"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Content</th>\n",
              "      <th>Category</th>\n",
              "      <th>Complete_Filename</th>\n",
              "      <th>id</th>\n",
              "      <th>News_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001.txt</td>\n",
              "      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n",
              "      <td>business</td>\n",
              "      <td>001.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002.txt</td>\n",
              "      <td>Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...</td>\n",
              "      <td>business</td>\n",
              "      <td>002.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>003.txt</td>\n",
              "      <td>Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...</td>\n",
              "      <td>business</td>\n",
              "      <td>003.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>1557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004.txt</td>\n",
              "      <td>High fuel prices hit BA's profits\\r\\n\\r\\nBriti...</td>\n",
              "      <td>business</td>\n",
              "      <td>004.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>005.txt</td>\n",
              "      <td>Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...</td>\n",
              "      <td>business</td>\n",
              "      <td>005.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>1575</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  File_Name                                            Content  ... id News_length\n",
              "0   001.txt  Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...  ...  1        2569\n",
              "1   002.txt  Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...  ...  1        2257\n",
              "2   003.txt  Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...  ...  1        1557\n",
              "3   004.txt  High fuel prices hit BA's profits\\r\\n\\r\\nBriti...  ...  1        2421\n",
              "4   005.txt  Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...  ...  1        1575\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZO3o9im6PxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "acb2aa13-d505-4cf9-8fc7-3aea30a06456"
      },
      "source": [
        "df.loc[1]['Content']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dollar gains on Greenspan speech\\r\\n\\r\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\r\\n\\r\\nAnd Alan Greenspan highlighted the US government\\'s willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan\\'s speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman\\'s taking a much more sanguine view on the current account deficit than he\\'s taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He\\'s taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\\r\\n\\r\\nWorries about the deficit concerns about China do, however, remain. China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing\\'s policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve\\'s decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US\\'s yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JYG2YDvAOoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"best_svc.pickle\",\"rb\") as model:\n",
        "  svc_model = pickle.load(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgDpMYtKAfc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"tfidf.pickle\",\"rb\") as data:\n",
        "  tfidf = pickle.load(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aglEUU596Ptj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c9e72cf5-cd32-4520-d917-9932d99fe27a"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "punctuation_signs = list(\"?:!.,;\")\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "def create_features_from_df(df):\n",
        "    \n",
        "    df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
        "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
        "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
        "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')\n",
        "    \n",
        "    df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()\n",
        "    \n",
        "    df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
        "    for punct_sign in punctuation_signs:\n",
        "        df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
        "        \n",
        "    df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
        "    \n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    nrows = len(df)\n",
        "    lemmatized_text_list = []\n",
        "    for row in range(0, nrows):\n",
        "\n",
        "        # Create an empty list containing lemmatized words\n",
        "        lemmatized_list = []\n",
        "        # Save the text and its words into an object\n",
        "        text = df.loc[row]['Content_Parsed_4']\n",
        "        text_words = text.split(\" \")\n",
        "        # Iterate through every word to lemmatize\n",
        "        for word in text_words:\n",
        "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
        "        # Join the list\n",
        "        lemmatized_text = \" \".join(lemmatized_list)\n",
        "        # Append to the list containing the texts\n",
        "        lemmatized_text_list.append(lemmatized_text)\n",
        "    \n",
        "    df['Content_Parsed_5'] = lemmatized_text_list\n",
        "    \n",
        "    df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
        "    for stop_word in stop_words:\n",
        "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "        df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
        "        \n",
        "    df = df['Content_Parsed_6']\n",
        "    df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
        "    \n",
        "    # TF-IDF\n",
        "    features = tfidf.transform(df).toarray()\n",
        "    \n",
        "    return features\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xsg0-S06Pps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category_codes = {\n",
        "    'business': 0,\n",
        "    'entertainment': 1,\n",
        "    'politics': 2,\n",
        "    'sport': 3,\n",
        "    'tech': 4,\n",
        "    'other':5\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS4_R4LFA7vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_category_name(category_id):\n",
        "    for category, id_ in category_codes.items():    \n",
        "        if id_ == category_id:\n",
        "            return category"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVmXzFbrBA-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_from_features(features):\n",
        "        \n",
        "    # Obtain the highest probability of the predictions for each article\n",
        "    predictions_proba = svc_model.predict_proba(features).max(axis=1)    \n",
        "    \n",
        "    # Predict using the input model\n",
        "    predictions_pre = svc_model.predict(features)\n",
        "\n",
        "    # Replace prediction with 6 if associated cond. probability less than threshold\n",
        "    predictions = []\n",
        "\n",
        "    for prob, cat in zip(predictions_proba, predictions_pre):\n",
        "        if prob > .65:\n",
        "            predictions.append(cat)\n",
        "        else:\n",
        "            predictions.append(5)\n",
        "\n",
        "    # Return result\n",
        "    categories = [get_category_name(x) for x in predictions]\n",
        "    \n",
        "    return categories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr92aeEQ6PmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complete_df(df, categories):\n",
        "    df['Prediction'] = categories\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cP_hjQoBYx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_news():\n",
        "  news_contents = \"In the men's freestyle, Rahul Aware (61kg) had clinched his first career Ranking Series title with a tactical 4-1 victory over Munir Aktas of Turkey. Utkarsh Kale had won bronze in the same category.\"\n",
        "  df_features = pd.DataFrame(\n",
        "         {'Content': [news_contents] \n",
        "        })\n",
        "  return df_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt033RbW6PgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_features = get_news()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hc0rutI6Pcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = create_features_from_df(df_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ph54awDh5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = predict_from_features(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0LIAlAfDmR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31725ede-fee9-4571-c2e2-c0278207b0b9"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sport']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}